{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7669b7bf",
   "metadata": {},
   "source": [
    "\n",
    "# Anomaly Detection in Telecommunications Networks\n",
    "\n",
    "This notebook contains code to preprocess data, classify anomalies, and visualize the results. Below are the requirements and instructions to run the notebook.\n",
    "\n",
    "## Authors\n",
    "\n",
    "This work was done by **Oussama Guemmar** and **Hatim Haffou**, supervised by **Mr. Said Kasli** at **Orange Morocco**.\n",
    "\n",
    "## Principle\n",
    "\n",
    "The principle of this work is to detect anomalies and degradations in telecommunication networks using statistical methods. The approach involves the following steps:\n",
    "\n",
    "1. Preprocessing the data: Cleaning and transforming the raw data to make it suitable for analysis.\n",
    "2. Applying rolling averages: Smoothing the data to identify trends and patterns over time.\n",
    "3. Statistical analysis: Using various statistical techniques to identify and classify anomalies in the data. This helps in distinguishing between normal and abnormal behavior in network traffic.\n",
    "4. Visualization: Plotting the data to visually inspect the anomalies and understand their impact on network performance.\n",
    "\n",
    "The ultimate goal is to improve network reliability and performance by identifying and addressing issues promptly.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Install the required packages using the following command:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Ensure you have the required dataframes from the specific database with the necessary columns.\n",
    "2. Run each cell sequentially to preprocess data, classify anomalies, and visualize the results.\n",
    "3. Company-specific data has been hidden for confidentiality.\n",
    "\n",
    "### Code Explanation and Execution\n",
    "\n",
    "Below are the explanations for each code cell:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b42905",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b52926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import timedelta\n",
    "from geopy.distance import great_circle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00611e9c",
   "metadata": {},
   "source": [
    "## Initialize Spark session and load data\n",
    "This cell initializes the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbff902",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b99f0c",
   "metadata": {},
   "source": [
    "## Data preprocessing function - hidden for confidentiality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea040a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    ...\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccff33",
   "metadata": {},
   "source": [
    "## Classify anomalies\n",
    "This function identifies anomalies in traffic data by calculating rolling statistics and applying threshold multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951653cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_anomalies(df, data_multiplier, cs_multiplier, classification_window):\n",
    "    window_spec = Window.partitionBy('CELL_NAME', 'WEEKDAY', 'HOUR').orderBy('DATETIME').rowsBetween(-classification_window + 1, 0)\n",
    "    df = df.withColumn('CS_RM', F.avg('TRAFIC_CS').over(window_spec))\n",
    "    df = df.withColumn('DATA_RM', F.avg('TRAFIC_DATA').over(window_spec))\n",
    "    df = df.withColumn('CS_STDDEV', F.stddev('TRAFIC_CS').over(window_spec))\n",
    "    df = df.withColumn('DATA_STDDEV', F.stddev('TRAFIC_DATA').over(window_spec))\n",
    "    df = df.na.fill({'TRAFIC_CS_ROLLING_STD': 0, 'TRAFIC_DATA_ROLLING_STD': 0})\n",
    "    df = df.withColumn('CS_CLA',\n",
    "                       F.when(F.col('TRAFIC_CS') > (F.col('CS_RM') + cs_multiplier * F.col('CS_STDDEV')), 'INCREASE')\n",
    "                       .when(F.col('TRAFIC_CS') < (F.col('CS_RM') - cs_multiplier * F.col('CS_STDDEV')), 'DEGRADATION')\n",
    "                       .otherwise('STABLE'))\n",
    "    df = df.withColumn('DATA_CLA',\n",
    "                       F.when(F.col('TRAFIC_DATA') > (F.col('DATA_RM') + data_multiplier * F.col('DATA_STDDEV')), 'INCREASE')\n",
    "                       .when(F.col('TRAFIC_DATA') < (F.col('DATA_RM') - data_multiplier * F.col('DATA_STDDEV')), 'DEGRADATION')\n",
    "                       .otherwise('STABLE'))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabc003",
   "metadata": {},
   "source": [
    "## Get traffic anomalies\n",
    "This cell extracts the traffic of anomalies cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7c4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomalies_traffic(df, anomaly_window, min_anomalies):\n",
    "    max_timestamp = df.agg(F.max('DATETIME')).collect()[0][0]\n",
    "    cutoff_datetime = max_timestamp - timedelta(hours=anomaly_window)\n",
    "    df_filtered = df.filter(F.col('DATETIME') >= cutoff_datetime)\n",
    "    classification_mapping = {'STABLE': 0, 'INCREASE': 1, 'DEGRADATION': 1}\n",
    "    mapping_expr = F.create_map([F.lit(x) for x in sum(classification_mapping.items(), ())])\n",
    "    df_filtered = df_filtered.withColumn('CS_CLANUM', mapping_expr[F.col('CS_CLA')])\n",
    "    df_filtered = df_filtered.withColumn('DATA_CLANUM', mapping_expr[F.col('DATA_CLA')])\n",
    "    window_spec = Window.partitionBy('CELL_NAME').orderBy('DATETIME').rowsBetween(-anomaly_window + 1, 0)\n",
    "    df_filtered = df_filtered.withColumn('CS_SCOUNT', F.sum('CS_CLANUM').over(window_spec))\n",
    "    df_filtered = df_filtered.withColumn('DATA_SCOUNT', F.sum('DATA_CLANUM').over(window_spec))\n",
    "    significant_periods = df_filtered.filter(\n",
    "        (F.col('CS_SCOUNT') >= min_anomalies) | \n",
    "        (F.col('DATA_SCOUNT') >= min_anomalies)\n",
    "    )\n",
    "    anomaly_cells = significant_periods.select('CELL_NAME').distinct()\n",
    "    full_traffic_data = df.join(anomaly_cells, on='CELL_NAME', how='inner')\n",
    "    return full_traffic_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d21d28",
   "metadata": {},
   "source": [
    "## Find facing cells\n",
    "This function finds neighboring cells that are facing the target cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290324cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_facing_cells(df, target_cell):\n",
    "    def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "        dlon = np.radians(lon2 - lon1)\n",
    "        lat1, lat2 = np.radians(lat1), np.radians(lat2)\n",
    "        x = np.sin(dlon) * np.cos(lat2)\n",
    "        y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "        return (np.degrees(np.arctan2(x, y)) + 360) % 360\n",
    "\n",
    "    def is_within_coverage(bearing, azimuth, coverage=60):\n",
    "        lower, upper = (azimuth - coverage) % 360, (azimuth + coverage) % 360\n",
    "        return np.where(lower < upper, (bearing >= lower) & (bearing <= upper), (bearing >= lower) | (bearing <= upper))\n",
    "    \n",
    "    if target_cell not in df['CELL_NAME'].values:\n",
    "        return pd.DataFrame()\n",
    "    target_row = df[df['CELL_NAME'] == target_cell].iloc[0]\n",
    "    max_distance_km = target_row['MAX_DISTANCE_BY_PROVINCE']\n",
    "    target_coords = np.array([target_row['LATITUDE'], target_row['LONGITUDE']])\n",
    "    target_azimuth = target_row['AZIMUTH']\n",
    "    cell_coords = df[['LATITUDE', 'LONGITUDE']].to_numpy()\n",
    "    target_coords = np.tile(target_coords, (len(cell_coords), 1))\n",
    "    distances = np.array([great_circle(target_coords[i], cell_coords[i]).kilometers for i in range(len(cell_coords))])\n",
    "    within_distance = distances <= max_distance_km\n",
    "    bearings_to_cell = calculate_bearing(target_coords[:, 0], target_coords[:, 1], cell_coords[:, 0], cell_coords[:, 1])\n",
    "    bearings_from_cell = calculate_bearing(cell_coords[:, 0], cell_coords[:, 1], target_coords[:, 0], target_coords[:, 1])\n",
    "    target_within_coverage = is_within_coverage(bearings_to_cell, target_azimuth)\n",
    "    cell_within_coverage = is_within_coverage(bearings_from_cell, df['AZIMUTH'].to_numpy())\n",
    "    valid_cells = within_distance & target_within_coverage & cell_within_coverage & (df['CELL_NAME'] != target_cell)\n",
    "    valid_cells_df = df[valid_cells]\n",
    "    same_site_same_azimuth = df[(df['SITE_BASE'] == target_row['SITE_BASE']) & (df['AZIMUTH'] == target_row['AZIMUTH']) & (df['CELL_NAME'] != target_cell)]\n",
    "    valid_cells_df = valid_cells_df[valid_cells_df['SITE_BASE'] != target_row['SITE_BASE']]\n",
    "    final_cells_df = pd.concat([valid_cells_df, same_site_same_azimuth]).drop_duplicates()\n",
    "    return final_cells_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74510c0",
   "metadata": {},
   "source": [
    "## Get neighbors' traffic data\n",
    "This function retrieves the traffic of neighboring cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68909ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors_traffic(classified_df, distinct_cells_pandas_df, geo_df, spark):\n",
    "    anomaly_neighbors_list = []\n",
    "    anomaly_geo_list = []\n",
    "\n",
    "    for _, row in distinct_cells_pandas_df.iterrows():\n",
    "        target_cell_name = row['CELL_NAME']\n",
    "        target_rows = geo_df[geo_df['CELL_NAME'] == target_cell_name]\n",
    "        \n",
    "        if not target_rows.empty:\n",
    "            target_row = target_rows.iloc[0]\n",
    "            target_latitude = target_row['LATITUDE']\n",
    "            target_longitude = target_row['LONGITUDE']\n",
    "            target_azimuth = target_row['AZIMUTH']\n",
    "\n",
    "            facing_cells = find_facing_cells(geo_df, target_cell_name)\n",
    "            if not facing_cells.empty:\n",
    "                for _, neighbor_row in facing_cells.iterrows():\n",
    "                    anomaly_neighbors_list.append({\n",
    "                        'ANOMALY_CELL': target_cell_name,\n",
    "                        'NEIGHBOR_CELL': neighbor_row['CELL_NAME']\n",
    "                    })\n",
    "                    anomaly_geo_list.append({\n",
    "                        'CELL_NAME': neighbor_row['CELL_NAME'],\n",
    "                        'LATITUDE': neighbor_row['LATITUDE'],\n",
    "                        'LONGITUDE': neighbor_row['LONGITUDE'],\n",
    "                        'AZIMUTH': neighbor_row['AZIMUTH']\n",
    "                    })\n",
    "            anomaly_geo_list.append({\n",
    "                'CELL_NAME': target_cell_name,\n",
    "                'LATITUDE': target_latitude,\n",
    "                'LONGITUDE': target_longitude,\n",
    "                'AZIMUTH': target_azimuth\n",
    "            })\n",
    "\n",
    "    anomaly_neighbors_df = pd.DataFrame(anomaly_neighbors_list)\n",
    "    anomaly_geo_df = pd.DataFrame(anomaly_geo_list).drop_duplicates()\n",
    "\n",
    "    anomaly_neighbors_spark_df = spark.createDataFrame(anomaly_neighbors_df)\n",
    "\n",
    "    neighbor_cells = anomaly_neighbors_spark_df.select(\"NEIGHBOR_CELL\").distinct()\n",
    "    traffic_neighbors_df = classified_df.join(neighbor_cells, classified_df['CELL_NAME'] == neighbor_cells['NEIGHBOR_CELL'], 'inner') \\\n",
    "                                        .drop(neighbor_cells['NEIGHBOR_CELL'])\n",
    "\n",
    "    return traffic_neighbors_df.toPandas(), anomaly_neighbors_df, anomaly_geo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce55aee",
   "metadata": {},
   "source": [
    "## Plot traffic data and classifications\n",
    "This cell plots the traffic data and classifications for the specified date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifications(df, start_date, end_date, cell_name=None, neighbors_df=None):\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    df_filtered = df[(df['DATETIME'] >= start_date) & \n",
    "                     (df['DATETIME'] <= end_date)]\n",
    "    \n",
    "    if cell_name is not None:\n",
    "        cells = [cell_name]\n",
    "        if neighbors_df is not None:\n",
    "            neighbors = neighbors_df[neighbors_df['ANOMALY_CELL'] == cell_name]['NEIGHBOR_CELL'].unique()\n",
    "            cells = cells + list(neighbors)\n",
    "        df_filtered = df_filtered[df_filtered['CELL_NAME'].isin(cells)]\n",
    "    else:\n",
    "        cells = df_filtered['CELL_NAME'].unique()\n",
    "    \n",
    "    for cell in cells:\n",
    "        cell_df = df_filtered[df_filtered['CELL_NAME'] == cell].copy()\n",
    "        cell_df = cell_df.sort_values('DATETIME')\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(cell_df['DATETIME'], cell_df['TRAFIC_DATA'], label='Data Traffic', linestyle='-', color='blue', alpha=0.5)\n",
    "        plt.plot(cell_df['DATETIME'], cell_df['TRAFIC_CS'], label='CS Traffic', linestyle='-', color='orange', alpha=0.5)\n",
    "        \n",
    "        for status, color in zip(['INCREASE', 'DEGRADATION'], ['red', 'green']):\n",
    "            data_status_periods = cell_df[cell_df['DATA_CLA'] == status]\n",
    "            cs_status_periods = cell_df[cell_df['CS_CLA'] == status]\n",
    "\n",
    "            plt.scatter(data_status_periods['DATETIME'], data_status_periods['TRAFIC_DATA'], c=color, label=f'Data {status}', zorder=5)\n",
    "            plt.scatter(cs_status_periods['DATETIME'], cs_status_periods['TRAFIC_CS'], c=color, label=f'CS {status}', zorder=5)\n",
    "\n",
    "        plt.title(f'Traffic Data and Classifications for Cell {cell}')\n",
    "        plt.xlabel('Datetime')\n",
    "        plt.ylabel('Traffic')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdc86b",
   "metadata": {},
   "source": [
    "## Execute the complete process and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2364a",
   "metadata": {},
   "source": [
    "### Assuming dataframes are loaded from a database and have the required columns\n",
    "Replace 'database_load_function' with the actual function to load data from your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19214c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_load_function():\n",
    "    ...\n",
    "    return traffic_df, geo_df\n",
    "\n",
    "traffic_df ,geo_df = database_load_function()\n",
    "\n",
    "processed_df = preprocess_data(traffic_df)\n",
    "classified_df = classify_anomalies(processed_df,cs_multiplier=1.5,data_multiplier=1.5,classification_window=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84266925",
   "metadata": {},
   "source": [
    "### Extract anomalies and get neighboring cells' traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71840a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_cells_with_anomalies_pandas_df = get_anomalies_traffic(classified_df, anomaly_window=12, min_anomalies=6)\n",
    "distinct_cells_pandas_df = traffic_cells_with_anomalies_pandas_df[['CELL_NAME']].drop_duplicates()\n",
    "traffic_neighbors_pandas_df, anomaly_neighbors_df, anomaly_geo_df = get_neighbors_traffic(classified_df, distinct_cells_pandas_df, geo_df, spark)\n",
    "full_traffic_data_pandas_df = pd.concat([traffic_cells_with_anomalies_pandas_df, traffic_neighbors_pandas_df]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f65754",
   "metadata": {},
   "source": [
    "### Count and list distinct cells with anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe658ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_cells_pandas_df = anomaly_neighbors_df[['ANOMALY_CELL']].drop_duplicates()\n",
    "distinct_cells_count = distinct_cells_pandas_df['ANOMALY_CELL'].nunique()\n",
    "print(f\"Number of distinct cells with anomalies: {distinct_cells_count}\")\n",
    "\n",
    "distinct_cells_list = distinct_cells_pandas_df['ANOMALY_CELL'].tolist()\n",
    "distinct_cells_string = ', '.join(map(str, distinct_cells_list))\n",
    "print(f\"Distinct cells with anomalies: {distinct_cells_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fbdd4",
   "metadata": {},
   "source": [
    "### Plot traffic data and classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifications(df=full_traffic_data_pandas_df, start_date='2024-05-01', end_date='2024-06-01', neighbors_df=anomaly_neighbors_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
